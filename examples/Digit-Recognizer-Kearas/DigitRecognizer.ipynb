{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer Using Keras \n",
    "\n",
    "Download dataset from <a href=\"https://www.kaggle.com/c/digit-recognizer/data\"> kaggle </a> and extract into current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AI\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(directory, train=True):    \n",
    "    if train:\n",
    "        data = np.genfromtxt(directory + r'train.csv', delimiter=',',\n",
    "                             dtype='float32', skip_header=1)\n",
    "        \n",
    "        # First column of train.csv is label then pixel 0-783\n",
    "        labels = data[:, 0]\n",
    "        data = data[:, 1:]        \n",
    "\n",
    "        # Pixel data must be reshaped to 2D array\n",
    "        data = np.reshape(data, (data.shape[0], 28, 28))\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        labels = to_categorical(labels, 10)     \n",
    "        \n",
    "        return data, labels\n",
    "    \n",
    "    # Note: test.csv has no label column    \n",
    "    else:\n",
    "        data = np.genfromtxt(directory + r'test.csv', delimiter=',',\n",
    "                             dtype='float32', skip_header=1)\n",
    "        data = np.reshape(data, (data.shape[0], 28, 28))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(data):\n",
    "    return data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_cnn(input_shape=(28, 28, 1), num_classes=10, lr=1E-3, dropout_rate=0.25):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=lr, decay=1e-6),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        1664      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 2,319,882\n",
      "Trainable params: 2,317,450\n",
      "Non-trainable params: 2,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = mnist_cnn(lr=1E-3, dropout_rate=0.4)\n",
    "\n",
    "# Setup data augmenter\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=2/28,\n",
    "    height_shift_range=2/28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Dimensions: (28, 28, 1)\n",
      "Number of Training Samples: 42000\n",
      "Number of Test Samples: 28000\n"
     ]
    }
   ],
   "source": [
    "# This should be changed to the directory of your datasets\n",
    "directory = r'data/'\n",
    "\n",
    "# Load and normalize train set\n",
    "X_train, Y_train = load_mnist(directory, train=True)\n",
    "X_train = preprocess_images(X_train)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "\n",
    "# Load and normalize test set\n",
    "X_test = load_mnist(directory, train=False)\n",
    "X_test = preprocess_images(X_test)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "print('Image Dimensions:', X_train.shape[1:])\n",
    "print('Number of Training Samples:', X_train.shape[0])\n",
    "print('Number of Test Samples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "657/656 [==============================] - 265s 403ms/step - loss: 0.2842 - acc: 0.9155\n",
      "Epoch 2/3\n",
      "657/656 [==============================] - 257s 391ms/step - loss: 0.1309 - acc: 0.9587\n",
      "Epoch 3/3\n",
      "657/656 [==============================] - 254s 387ms/step - loss: 0.1032 - acc: 0.9696\n",
      "Epoch 1/3\n",
      "657/656 [==============================] - 255s 387ms/step - loss: 0.0922 - acc: 0.9714\n",
      "Epoch 2/3\n",
      "657/656 [==============================] - 246s 374ms/step - loss: 0.0860 - acc: 0.9734\n",
      "Epoch 3/3\n",
      "657/656 [==============================] - 248s 378ms/step - loss: 0.0810 - acc: 0.9758\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(2):\n",
    "    lr_anneal = ReduceLROnPlateau(monitor='loss', patience=3, factor=0.1)\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n",
    "                        epochs=3, steps_per_epoch=len(X_train) / 64,\n",
    "                        callbacks=[lr_anneal])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels using each model\n",
    "labels = []\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    labels.append(y_pred)\n",
    "    \n",
    "# Perform ensemble classification voting\n",
    "labels = np.array(labels)\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "labels = scipy.stats.mode(labels, axis=-1)[0]\n",
    "labels = np.squeeze(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save this model\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model from disk\n"
     ]
    }
   ],
   "source": [
    "#Load the model for production\n",
    "from keras.models import model_from_json\n",
    "import tensorflow as tf\n",
    "json_file = open('model.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "#load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded Model from disk\")\n",
    "#compile and evaluate loaded model\n",
    "lr=1E-3\n",
    "loaded_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=lr, decay=1e-6),metrics=['accuracy'])\n",
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "def imageprepare(argv):\n",
    "    \"\"\"\n",
    "    This function returns the pixel values.\n",
    "    The imput is a png file location.\n",
    "    \"\"\"\n",
    "    im = Image.open(argv).convert('L')\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    newImage = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if (nheight == 0):  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Heigth becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if (nwidth == 0):  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "    #newImage.save(\"sample.png\")\n",
    "\n",
    "    tv = list(newImage.getdata())  # get pixel values\n",
    "\n",
    "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    "    \n",
    "    \n",
    "    return tva\n",
    "\n",
    "\n",
    "def predict():\n",
    "    x=imageprepare('./img1.png')#file path here\n",
    "    x=np.asarray(x).reshape(1,28,28,1)\n",
    "    out = loaded_model.predict(x)\n",
    "    #print(out)\n",
    "    print(np.argmax(out, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
